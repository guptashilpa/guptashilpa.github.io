[
  {
    "path": "posts/2021-11-01-aa-experiments/",
    "title": "Experience with Authentic Assessment in a Data Science Classroom",
    "description": "Data Science is the art of turning data into information. It attracts learners from various disciplines and different levels of fluency with statistical concepts. This paper describes the experience of using systems thinking to design assessments for a mixed skill classroom on data science foundations using approaches from student-centric pedagogies, such as equity grading, project based learning and authentic assessment. Both formative and summative assessments jointly provide experiential understanding of statistical concepts and result in building programming literacy skills and holistic understanding of a data analysis project. Further, leveraging student's agency and interest in their domains of choice by bringing their own data results in a more engaged learning environment. Unstructured assessments also have an unintended consequence of uncovering challenging aspects of the topics which would have been masked by structured problem solving. This feedback loop aids the instructor in creating more meaningful scaffolding assignments to make the learning deeper.",
    "author": [
      {
        "name": "Shilpa Gupta, Ph.D.",
        "url": {}
      }
    ],
    "date": "2022-04-19",
    "categories": [],
    "contents": "\nIntroduction\nTwo forces, ubiquity of data collection of various types (such as\nimages, video, sound, among others) and an increase in computing power\nto transform those complex data types to information, has accelerated\nthe use of advanced black box models. There is now an urgent need to\nupskill the future data scientist and ML engineers not only with how to\nperform data analysis but also an intuitive understanding of the inner\nworkings of the black box approaches. This starts with understanding the\nsimplest of approaches involved in an end-to-end data analysis project.\nAt San Jose State University, we have re-imagined the applied statistics\nand probability for engineers course to include foundational concepts\nfrom matrix algebra and optimization disciplines. Expanding the topics\nin this way presents an opportunity for students to see the process of\ntranslating real world problems into an analytic form, subsequently\nfollowed by collecting and preparing data for statistical inference and\nprediction. Typically the matrix decomposition and factorization based\napproaches for feature engineering and inference are taught as a\nseparate semester-long course. That is also true for calculus based\noptimization routines for parameter estimation. Often different teaching\nstyles and depth of topics covered results in students developing weak\nconnections. We are not debating the importance of having semester-long\ncourses for a deep learning of topics, instead argue for at least one\ncourse in the curriculum that views the representative problem of the\ncurriculum from a systems engineer perspective [@lucas2016thinking] and provide\nend to end understanding of the approaches involved in data analysis. We\nhave observed that this approach makes the learners develop stronger\nconnections among the concepts and prepare them better for deep\ntreatment of the topics.\nThe new course hence developed, ’Math Foundations for Decision and\nData Science, is a core course for the pursuants of graduate degree in\nEngineering Management, Industrial & Systems Engineering and\nArtificial Intelligence. It would become immediately apparent to the\nreader that the career goals of the students pursuing these degrees\nwould be different. There is also a strong possibility that the students\nenter the program with different levels of exposure to these concepts.\nThe commonality among the students is their desire to pursue\nprofessional careers using data skills.\nIn recent years there has been growth in adoption of open source\nsoftware and availability of public domain data. This has created an\ninteresting scenario of using evidence based data science skills for\nscreening candidates [@udacity_2021]. Hiring companies now\nregularly do hands-on problem solving, and occasionally host data\ncompetitions on platforms like Kaggle to reduce the pool of applicants.\nMerely studying the course is no longer sufficient, having experiential\nevidence either as an active github repository or long form written\nblogs or a rudimentary web application is becoming increasingly\ndesirable.\nThe challenge then as an instructor becomes organizing the content\nand designing assessments that would not only meet the needs of the\nmixed experience level students but also enable them to meet the demands\nof the companies hiring for data skills.\nAuthentic Assessment\nThe need of the students is to develop hands-on understanding of\napproaches for turning unstructured data into information. This includes\nbeing able to manipulate data using statistical software and draw\ninferences based on the understanding of the approaches. To translate\nthat into a lesson plan, it is essential first to catalog concepts\nstudents need to master and assessments to evaluate if that objective\nwas achieved. That is in essence the philosophy of reverse instructional\ndesign, [@wiggins2005understanding].\nAmong the different types of assessments, the first step is to design\nthe high stakes cumulative assessments (also known as summative\nassessment) subsequently followed by designing low stakes assessments\nthat would serve as scaffolding assessments (or formative assessment)\nand eventually resulting in development of lesson plans. A more nuanced\ndistinction among the different types of assessments can be found in the\npaper [@harlen1997assessment].\nAn important guiding concept in rethinking course delivery is\nauthentic assessment [@wiggins1989true], [@wiggins1998educative]. It has\nbeen explained as , “… representative challenges within a given\ndiscipline. They are designed to emphasize realistic (but fair)\ncomplexity; they stress more on depth over breadth. In doing so, they\nmust necessarily involve somewhat ambiguous, ill-structured tasks or\nproblems.” [@wiggins_27c_2022]. The article\nlists various characteristics of authentic tasks. In particular it\nclarifies distinction between hands-on and real world learning by\nstating that the latter focuses on the impact of the solution versus the\nformer focuses on creating or building artifacts as a demonstration of\nunderstanding. There has been some work of interpreting what authentic\nassessment means for a statistics class over the years [@garfield2000assessment],\n[@joan1994beyond], [@garfield2007students], [@libman2010integrating]. The\nneeds of the industry and the students have evolved since then. In this\npaper we revisit the assessments ensuring the assessments meet the\nguidelines [@chance1997experiences] among\nothers of timely and constructive feedback and opportunity for\nreflection among the students.\nAnother useful concept is the connection between knowledge, skill and\ncourse learning objectives [@toikkanen_2016], illustrated in\nFigure 1. Developing hands-on data analysis skill is crucial not only\nfor demonstration of the knowledge but also understanding of the\nconcepts, succinctly put as “students learn best when they practice and\nperform on their own” [@tishkovskaya2012statistical].\nDesigning Assessment\nAn authentic cumulative assessment for a data analysis course is a\ndata analysis project. For learners getting started in the field that\nwould include understanding data using descriptive statistics,\nvisualizations of trends and patterns, data quality checks, feature\npreprocessing, statistical inference to test hypotheses and a predictive\nmodel (simple linear regression or logistic regression). For the course,\nthe project was split into three intermediate submissions, the first one\naround data selection and data quality checks, second on performing\nexploratory data analysis and generating hypotheses, third using\nstatistical inference and predictive models. To make it authentic,\nstudents were given the agency to select a dataset that interested them\n[@opendata_2021].The tasks were\ninherently unstructured due to the fact that the students picked a\ndifferent dataset with different challenges, but the task was scaffolded\nby providing a rubric to set expectations and providing guiding\nquestions to walk them through the thought process of the analysis. Data\nanalysis projects are part of data science courses, and are usually\nimplemented as a group project[@ccetinkaya2021fresh]. We\npropose individual data analysis projects with peer evaluations for\nselected milestones.\nAware of the potential failings of unstructured projects [@kirschner2010minimal], we\ndeveloped formative assessments that scaffolded skills required for the\nfinal project but on preprocessed data and better defined tasks. This\nenabled the students to focus on the approach and interpretation one\ntask at a time.\nTo balance experiential learning, the assessments also needed to\nevaluate for understanding of concepts. For this course, that was\nimplemented as a multiple choice quiz. This form of assessment was\nfocused on topic specific understanding so it was delivered after the\nend of each of the four modules, namely linear algebra, probability,\nstatistics & optimization.\nFinally the lessons were then designed to demonstrate the mathematics\nof the approaches and illustrated the concepts with applications and\nin-class worked-out examples.\nEquity Grading\nThe change in course assignments was designed and implemented during\nthe 2020 COVID pandemic and inequity in the circumstances made it urgent\nto address the inequity in student learning. Elements of equity grading\n[@feldman2018grading] were\nincorporated by allowing resubmission of the assignments and quizzes to\nimprove understanding. The resubmissions specifically focussed on fixing\nthe gaps in understanding. It also included a reflection component for\nstudents to become aware of their learning patterns. The other aspect of\nequity grading was removing the penalty for late submissions. This\ndrastically reduced missing submissions. Homework assignments and\nformative submissions focused on practicing statistical programming\nlanguage such as R/Python skills. Creating milestones for the project\nsubmissions, reduced the cognitive load of planning for an end to end\ndata analysis project.\nPedagogy Effectiveness\nFrom the informal discussion with the students during class and\noffice hour, the students reported finding value in the project\ncomponent. Every semester a survey is sent to all the students to\nevaluate teacher and teaching effectiveness. The survey has ten\nquestions and one among them is “Used assignments that enhanced\nlearning.” The score on this question was used to compare against the\ndelivery of the same content in two semesters. One with the assessments\ndetailed in previous sections and one without. The score for the end of\nsemester teaching evaluation question increased from 4.4 to 4.6 after\nimplementation of the assessments described in the paper.\nObservation and Reflection\nUnstructured assignments are hard. That is not surprising for anyone\nwho is familiar with cognitive psychology. Novice learners lack useful\nmental models to perform tasks efficiently as an expert [@daley1999novice].\nFurther most students entering this class have been part of systems\nwhere they were used to being told what to do. This was reflected in\nsome students having difficulty in making decisions as simple as\nselecting a dataset. Chunking up the summative assignments that students\nsubmit over the course of a semester for feedback and improvement before\nsubmitting the final project at the end of the semester improved the\nquality and also reduced the stress level among the students.\nBy introducing authentic assignments, the challenge often encountered\nin a mixed skills class room was addressed. Students on both sides of\nthe spectrum of fluency in programming skills and prior exposure to\nconcepts felt challenged and exhibited learning. It was observed that\nthe students chose and analyzed the dataset according to their skill and\nexperience level in the field. Students with prior exposure picked a\nmore nuanced dataset to challenge themselves and relatively novice\nstudents were able to exhibit similar approaches on a simpler\ndataset.\nMaking the assignments more unstructured made it apparent as an\ninstructor what concepts students struggle with the most[@ijeh2012competent]. Providing\nthe opportunity to work on the corrections for credit, motivated\nstudents to fix their gap in learning. The project gave them the\nautonomy to apply the concepts they were learning in the class on the\ndataset of their choice further increased their engagement and\nunderstanding of end-to-end data analysis projects.\nNext Steps\nDesigning the assessments as detailed in the previous section,\nprovided a novel perspective into gaps which the structured assignments\noften miss. Traditional assignments and exams provide well defined\nproblem statements, where the student most often is tested on how they\nperform the analysis but less frequently on formulating the analytic\nproblem. This enabled the instructor to develop scaffolding tasks and\nformative assessments to focus on particular skills. Additionally there\nare other pedagogies that could be incorporated to develop the student\nto become an independent learner. Another potential next step could be\nto give the choice to the student to collect data for analysis [@anderson2019turning].\nInsights and Recommendations\nA systems thinking approach to designing the assessments for a\nfoundation course can be extended to other courses as well. The key\ndifference between novice learners and experienced learners [@daley1999novice]\nis the richness of the mental models and density of connections.\nExposing learners to the end to end process of data analysis, including\ndata preprocessing, asking questions and translating that into analytic\nquestions, tasks which are often missing from traditional assessments,\nprovides a more authentic experience. A hands-on appreciation of the\niterative nature of data analysis projects makes them better prepared\nfor the real world tasks. Applying the concepts using open source\nprogramming languages adds to their skills repertoire.\nThe key insight and recommendation from the experience has been to\nhave at least one course in the curriculum that uses the systems\nthinking approach to the tasks in the field. A survey conducted among\ndata science educators found that systems thinking is often omitted from\nthe data science curriculum[@schwab2021data]. That is a lost\nopportunity to prepare learners to be holistic thinkers and better\nprepared to enter a professional world,[@busteed2019aren]. This gap can be\nreduced with assignments that provide a taste of the authentic\nchallenges in the discipline.\nConclusion\nOne might ask the question - What place does an applied project have\nin a Math Foundations course? Shouldn’t the learning objective be to\ndevelop mathematical fluency underlying machine learning approaches?\nThat is an important question and the answer resides in the learners to\nwhom this course is targeted. As briefly mentioned in the introduction,\nthe course attracts students from a variety of disciplines and different\nexperience levels. Most of the students are looking to apply their\neducation via internship or a full time job in the field. Employing a\nsystems thinking approach in organizing the topics and designing the\nassessments has addressed the challenges of keeping mixed skills level\nclass engaged, equipping the students with experiential learning using\nauthentic tasks to become contributing professionals and tying\nfoundational concepts from linear algebra, probability, statistics and\noptimization, each justifiably a semester worth topic, into one cohesive\nwhole. The lectures focus on providing the mathematical intuition for\nthe approaches and the assignments provide the much needed practice of\napplication of those concepts.\nApprenticeship has always been an excellent way to gain knowledge as\nthey expose the student to not only learning the craft better but also\nlearning the context. In the classroom, sometimes the context is lost or\nthe analysis is so tunneled that the onus is left on the student to\nfigure out the whole life cycle. Systems thinking of designing authentic\nassessment and equity based grading has given the student experiential\nunderstanding of end-to-end data analysis projects.\nReferences [1] 2021. Data science jobs continue to be in-demand.\nUdacity.\n[2] Busteed, B. 2019. Why aren’t graduates ready for work? They’re the\nleast working generation in US history. Forbes. (2019).\n[3] Daley, B.J. 1999. Novice to expert: An exploration of how\nprofessionals learn. Adult education quarterly. 49, 4 (1999),\n133–147.\n[4] Feldman, J. 2018. Grading for equity: What it is, why it matters,\nand how it can transform schools and classrooms. Corwin Press.\n[5] Harlen, W. and James, M. 1997. Assessment and learning: Differences\nand relationships between for- mative and summative assessment.\nAssessment in education: Principles, policy & practice. 4, 3 (1997),\n365–379.\n[6] Kirschner, P.A. et al. 2010. Why minimal guid- ance during\ninstruction does not work: An analysis of the failure of\nconstructivists. Based Teaching Work: An Analysis of the Failure of\nConstructivist, Discov- ery, Problem-Based, Experiential, and\nInquiry-Based Teaching,(November 2014). (2010), 37–41.\n[7] Lucas, B. and Hanson, J. 2016. Thinking like an engineer: Using\nengineering habits of mind and sig- nature pedagogies to redesign\nengineering education. (2016).\n[8] Schwab-McCoy, A. et al. 2021. Data science in 2020: Computing,\ncurricula, and challenges for the next 10 years. Journal of Statistics\nand Data Science Educa- tion. 29, sup1 (2021), S40–S50. [9] Staff, T.\nand Wiggins, A.T.A.G. 2022. 27 character- istics of authentic\nassessment. TeachThought.\n[10] Tishkovskaya, S. and Lancaster, G.A. 2012. Statistical education in\nthe 21st century: A review of chal- lenges, teaching innovations and\nstrategies for reform. Journal of Statistics Education. 20, 2 (2012).\n[11] Toikkanen, T. 2016. Focus on skill mastery, not knowledge\nacquisition. Medium. LifeLearn. [12] Vik Paruchuri Vik is the CEO, A.\nthe author and Dataquest., F. of 2021. 21 places to find free datasets\nfor data science projects. Dataquest. [13] Wiggins, G. 1989. A true\ntest: Toward more authen- tic and equitable assessment. Phi Delta\nKappan. 70, 9 (1989), 703–713. [14] Wiggins, G. 1998. Educative\nassessment. Design- ing Assessments To Inform; Improve Student Perfor-\nmance. [15] Wiggins, G. et al. 2005. Understanding by design. Ascd. [16]\nWiggins, G. and McTighe, J. 1998. What is back- ward design.\nUnderstanding by design. 1, (1998), 7–19. [17] Joan B, G. (1994). Beyond\ntesting and grading: Using assessment to improve student learning.\nJournal of statistics education, 2(1). [18] Garfield, J., & Chance,\nB. (2000). Assessment in statistics education: Issues and challenges.\nMathematical Thinking and Learning, 2(1-2), 99-125. [19] Chance, B. L.\n(1997). Experiences with authentic assessment techniques in an\nintroductory statistics course. Journal of Statistics Education, 5(3).\n[20] Garfield, J., & Ben‐Zvi, D. (2007). How students learn\nstatistics revisited: A current review of research on teaching and\nlearning statistics. International statistical review, 75(3), 372-396.\n[21]Libman, Z. (2010). Integrating real-life data analysis in teaching\ndescriptive statistics: A constructivist approach. Journal of Statistics\nEducation, 18(1). [22] Ijeh, S. B. (2012). How competent mathematics\nteachers develop pedagogical content knowledge in statistics teaching\n(Doctoral dissertation, University of Pretoria). [23] Anderson, J. S.,\n& Williams, S. K. (2019). Turning data into better decision making:\nAsking questions, collecting and analyzing data in a personal analytics\nproject. Decision Sciences Journal of Innovative Education, 17(2),\n126-145.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-29T12:26:40-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-13-sudoku-pygame/",
    "title": "sudoku-pygame",
    "description": "Experiences in building a sudoku in python with a 8th grader",
    "author": [
      {
        "name": "Shilpa Gupta",
        "url": {}
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\nAs part of More Active Girls in Computing I worked with an 8th grader to build a python game. After much exploring[1], my mentee settled on Sudoku. Now began the real challenge of building the game in a new language in 8 weeks with roughly 24 hours total time. We started with doing an internet search of various Sudoku solutions[2]. Quite a few focused on algorithms for solving Sudoku.\nWhile it was quite attractive to build a solver for Sudoku, we prioritized building a GUI for Sudoku and mimicking solving Sudoku as we would on paper. For that we quickly narrowed to the work of Trevor Appleton. The blog did an excellent job of walking through the various steps. We spent a few weeks going through the code and understanding the pieces. We quickly realized that a. the code only looked at a 9x9 grid with hard coded values and b. It started with a blank puzzle. So for the remaining weeks we focused on making the code generic for a 4x4 grid, adding a GUI for choosing the game, adding a few numbers to have a starter puzzle.\nThe code is documented here https://github.com/guptashilpa/sudoku-pygame\nReferences\n[1]\nhttps://www.pygame.org/docs/tut/MakeGames.html https://www.pygame.org/docs/tut/tom_games2.html#makegames-2-1 https://inventwithpython.com/makinggames.pdf\n[2]\nhttps://norvig.com/sudoku.html https://stackoverflow.com/questions/1697334/algorithm-for-solving-sudoku https://www.geeksforgeeks.org/building-and-visualizing-sudoku-game-using-pygame/ https://www.techwithtim.net/tutorials/python-programming/sudoku-solver-backtracking/ http://www.ams.org/notices/200904/rtx090400460p.pdf https://see.stanford.edu/materials/icspacs106b/H19-RecBacktrackExamples.pdf\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-13T14:42:29-07:00",
    "input_file": {}
  }
]
