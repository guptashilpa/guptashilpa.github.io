[
  {
    "path": "posts/2022-11-10-encouraging-students-to-read/",
    "title": "Encouraging Students to Read",
    "description": "Using Reading Apprenticeship framework to encourage students to deepen their learning by reading",
    "author": [
      {
        "name": "Shilpa Gupta, Ph.D.",
        "url": {
          "https://rpubs.com/guptashilpa/964846": {}
        }
      }
    ],
    "date": "2022-11-10",
    "categories": [],
    "contents": "\nThese slides were created to introduce Reading Apprenticeship Framework to faculty colleagues.\nSlides\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-10T14:44:51-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-01-aa-experiments/",
    "title": "Experience with Authentic Assessment in a Data Science Classroom",
    "description": "Data Science is the art of turning data into information. It attracts learners from various disciplines and different levels of fluency with statistical concepts. This paper describes the experience of using systems thinking to design assessments for a mixed skill classroom on data science foundations using approaches from student-centric pedagogies, such as equity grading, project based learning and authentic assessment. Both formative and summative assessments jointly provide experiential understanding of statistical concepts and result in building programming literacy skills and holistic understanding of a data analysis project. Further, leveraging student's agency and interest in their domains of choice by bringing their own data results in a more engaged learning environment. Unstructured assessments also have an unintended consequence of uncovering challenging aspects of the topics which would have been masked by structured problem solving. This feedback loop aids the instructor in creating more meaningful scaffolding assignments to make the learning deeper.",
    "author": [
      {
        "name": "Shilpa Gupta, Ph.D.",
        "url": {}
      }
    ],
    "date": "2022-04-19",
    "categories": [],
    "contents": "\nKeywords\nData Science, Authentic Assessment, Equity Grading, Project Based\nLearning, Formative and Summative Assessments\nIntroduction\nTwo forces, ubiquity of data collection of various types (such as\nimages, video, sound, among others) and an increase in computing power\nto transform those complex data types to information, has accelerated\nthe use of advanced black box models. There is now an urgent need to\nupskill the future data scientist and ML engineers not only with how to\nperform data analysis but also an intuitive understanding of the inner\nworkings of the black box approaches. This starts with understanding the\nsimplest of approaches involved in an end-to-end data analysis project.\nAt San Jose State University, we have re-imagined the applied statistics\nand probability for engineers course to include foundational concepts\nfrom matrix algebra and optimization disciplines. Expanding the topics\nin this way presents an opportunity for students to see the process of\ntranslating real world problems into an analytic form, subsequently\nfollowed by collecting and preparing data to statistical inference and\nprediction. Typically the matrix decomposition and factorization based\napproaches for feature engineering and inference are taught as a\nseparate semester-long course. That is also true for calculus based\noptimization routines for parameter estimation. Often different teaching\nstyles and depth of topics covered results in students developing weak\nconnections. We are not debating the importance of having semester-long\ncourses for a deep learning of topics, instead argue for at least one\ncourse in the curriculum that views the representative problem of the\ncurriculum from a systems engineer perspective [15] and provide end to end understanding\nof the approaches involved in data analysis. We have observed that this\napproach makes the learners develop stronger connections among the\nconcepts and prepare them better for deep treatment of the topics.\nThe new course hence developed, ’Math Foundations for Decision and\nData Science, is a core course for the pursuants of graduate degree in\nEngineering Management, Industrial & Systems Engineering and\nArtificial Intelligence. It would become immediately apparent to the\nreader that the career goals of the students pursuing these degrees\nwould be different. There is also a strong possibility that the students\nenter the program with different levels of exposure to these concepts.\nThe commonality among the students is their desire to pursue\nprofessional careers using data skills.\nIn recent years there has been growth in adoption of open source\nsoftware and availability of public domain data. This has created an\ninteresting scenario of using evidence based data science skills for\nscreening candidates [1]. Hiring companies now regularly do\nhands-on problem solving, and occasionally host data competitions on\nplatforms like Kaggle to reduce the pool of applicants. Merely studying\nthe course is no longer sufficient, having experiential evidence either\nas an active github repository or long form written blogs or a\nrudimentary web application is becoming increasingly desirable.\nThe challenge then as an instructor becomes organizing the content\nand designing assessments that would not only meet the needs of the\nmixed experience level students but also enable them to meet the demands\nof the companies hiring for data skills.\nAuthentic Assessment\nThe need of the students is to develop hands-on understanding of\napproaches for turning unstructured data into information. This includes\nbeing able to manipulate data using statistical software and draw\ninferences based on the understanding of the approaches. To translate\nthat into a lesson plan, it is essential first to catalog concepts\nstudents need to master and assessments to evaluate if that objective\nwas achieved. That is in essence the philosophy of reverse instructional\ndesign, [23]. Among the different types of\nassessments, the first step is to design the high stakes cumulative\nassessments (also known as summative assessment) subsequently followed\nby designing low stakes assessments that would serve as scaffolding\nassessments (or formative assessment) and eventually resulting in\ndevelopment of lesson plans. A more nuanced distinction among the\ndifferent types of assessments can be found in the paper [10].\nAn important guiding concept in rethinking course delivery is\nauthentic assessment [21], [22]. It has been explained as , “…\nrepresentative challenges within a given discipline. They are designed\nto emphasize realistic (but fair) complexity; they stress more on depth\nover breadth. In doing so, they must necessarily involve somewhat\nambiguous, ill-structured tasks or problems.” [17]. The article lists various\ncharacteristics of authentic tasks. In particular it clarifies\ndistinction between hands-on and real world learning by stating that the\nlatter focuses on the impact of the solution versus the former focuses\non creating or building artifacts as a demonstration of understanding.\nThere has been some work of interpreting what authentic assessment means\nfor a statistics class over the years [9],\n[12], [8],\n[14].\nThe needs of the industry and the students have evolved since then. In\nthis paper we revisit the assessments ensuring the assessments meet the\nguidelines [5]\namong others of timely and constructive feedback and opportunity for\nreflection among the students.\nAnother useful concept is the connection between knowledge, skill and\ncourse learning objectives [19], illustrated in Figure 1. Developing\nhands-on data analysis skill is crucial not only for demonstration of\nthe knowledge but also understanding of the concepts, succinctly put as\n“students learn best when they practice and perform on their own” [18].\n\n\n\nFigure 1: Relaionship between knowledge, skill and course\nlearning outcome from Focus on skill mastery, not knowledge acquisition\nby Toikkanen\n\n\n\nDesigning Assessments\nAn authentic cumulative assessment for a data analysis course is a\ndata analysis project. For learners getting started in the field that\nwould include understanding data using descriptive statistics,\nvisualizations of trends and patterns, data quality checks, feature\npreprocessing, statistical inference to test hypotheses and a predictive\nmodel (simple linear regression or logistic regression). For the course,\nthe project was split into three intermediate submissions, the first one\naround data selection and data quality checks, second on performing\nexploratory data analysis and generating hypotheses, third using\nstatistical inference and predictive models. To make it authentic,\nstudents were given the agency to select a dataset that interested them\n[20].The tasks\nwere inherently unstructured due to the fact that the students picked a\ndifferent dataset with different challenges, but the task was scaffolded\nby providing a rubric to set expectations and providing guiding\nquestions to walk them through the thought process of the analysis. Data\nanalysis projects are part of data science courses, and are usually\nimplemented as a group project[4]. We propose individual data analysis\nprojects with peer evaluations for selected milestones.\nAware of the potential failings of unstructured projects [13], we\ndeveloped formative assessments that scaffolded skills required for the\nfinal project but on preprocessed data and better defined tasks. This\nenabled the students to focus on the approach and interpretation one\ntask at a time.\nTo balance experiential learning, the assessments also needed to\nevaluate for understanding of concepts. For this course, that was\nimplemented as a multiple choice quiz. This form of assessment was\nfocused on topic specific understanding so it was delivered after the\nend of each of the four modules, namely linear algebra, probability,\nstatistics & optimization.\nFinally the lessons were then designed to demonstrate the mathematics\nof the approaches and illustrated the concepts with applications and\nin-class worked-out examples.\nEquity grading\nThe change in course assignments was designed and implemented during\nthe 2020 COVID pandemic and inequity in the circumstances made it urgent\nto address the inequity in student learning. Elements of equity grading\n[7] were\nincorporated by allowing resubmission of the assignments and quizzes to\nimprove understanding. The resubmissions specifically focussed on fixing\nthe gaps in understanding. It also included a reflection component for\nstudents to become aware of their learning patterns. The other aspect of\nequity grading was removing the penalty for late submissions. This\ndrastically reduced missing submissions. Homework assignments and\nformative submissions focused on practicing statistical programming\nlanguage such as R/Python skills. Creating milestones for the project\nsubmissions, reduced the cognitive load of planning for an end to end\ndata analysis project.\nPedagogy Effectiveness\nFrom the informal discussion with the students during class and\noffice hour, the students reported finding value in the project\ncomponent. Every semester a survey is sent to all the students to\nevaluate teacher and teaching effectiveness. The survey has ten\nquestions and one among them is “Used assignments that enhanced\nlearning.” The score on this question was used to compare against the\ndelivery of the same content in two semesters. One with the assessments\ndetailed in previous sections and one without. The score for the end of\nsemester teaching evaluation question increased from 4.4 to 4.6 after\nimplementation of the assessments described in the paper.\nObservation and Reflection\nUnstructured assignments are hard. That is not surprising for anyone\nwho is familiar with cognitive psychology. Novice learners lack useful\nmental models to perform tasks efficiently as an expert [6]. Further\nmost students entering this class have been part of systems where they\nwere used to being told what to do. This was reflected in some students\nhaving difficulty in making decisions as simple as selecting a dataset.\nChunking up the summative assignments that students submit over the\ncourse of a semester for feedback and improvement before submitting the\nfinal project at the end of the semester improved the quality and also\nreduced the stress level among the students.\nBy introducing authentic assignments, the challenge often encountered\nin a mixed skills class room was addressed. Students on both sides of\nthe spectrum of fluency in programming skills and prior exposure to\nconcepts felt challenged and exhibited learning. It was observed that\nthe students chose and analyzed the dataset according to their skill and\nexperience level in the field. Students with prior exposure picked a\nmore nuanced dataset to challenge themselves and relatively novice\nstudents were able to exhibit similar approaches on a simpler\ndataset.\nMaking the assignments more unstructured made it apparent as an\ninstructor what concepts students struggle with the most[11].\nProviding the opportunity to work on the corrections for credit,\nmotivated students to fix their gap in learning. The project gave them\nthe autonomy to apply the concepts they were learning in the class on\nthe dataset of their choice further increased their engagement and\nunderstanding of end-to-end data analysis projects.\nNext Steps\nDesigning the assessments as detailed in the previous section,\nprovided a novel perspective into gaps which the structured assignments\noften miss. Traditional assignments and exams provide well defined\nproblem statements, where the student most often is tested on how they\nperform the analysis but less frequently on formulating the analytic\nproblem. This enabled the instructor to develop scaffolding tasks and\nformative assessments to focus on particular skills. Additionally there\nare other pedagogies that could be incorporated to develop the student\nto become an independent learner. Another potential next step could be\nto give the choice to the student to collect data for analysis [2].\nInsights and Recommendations\nA systems thinking approach to designing the assessments for a\nfoundation course can be extended to other courses as well. The key\ndifference between novice learners and experienced learners [6] is the\nrichness of the mental models and density of connections. Exposing\nlearners to the end to end process of data analysis, including data\npreprocessing, asking questions and translating that into analytic\nquestions, tasks which are often missing from traditional assessments,\nprovides a more authentic experience. A hands-on appreciation of the\niterative nature of data analysis projects makes them better prepared\nfor the real world tasks. Applying the concepts using open source\nprogramming languages adds to their skills repertoire.\nThe key insight and recommendation from the experience has been to\nhave at least one course in the curriculum that uses the systems\nthinking approach to the tasks in the field. A survey conducted among\ndata science educators found that systems thinking is often omitted from\nthe data science curriculum[16]. That is a lost opportunity to\nprepare learners to be holistic thinkers and better prepared to enter a\nprofessional world,[3]. This gap can be reduced with\nassignments that provide a taste of the authentic challenges in the\ndiscipline.\nConclusion\nOne might ask the question - What place does an applied project have\nin a Math Foundations course? Shouldn’t the learning objective be to\ndevelop mathematical fluency underlying machine learning approaches?\nThat is an important question and the answer resides in the learners to\nwhom this course is targeted. As briefly mentioned in the introduction,\nthe course attracts students from a variety of disciplines and different\nexperience levels. Most of the students are looking to apply their\neducation via internship or a full time job in the field. Employing a\nsystems thinking approach in organizing the topics and designing the\nassessments has addressed the challenges of keeping mixed skills level\nclass engaged, equipping the students with experiential learning using\nauthentic tasks to become contributing professionals and tying\nfoundational concepts from linear algebra, probability, statistics and\noptimization, each justifiably a semester worth topic, into one cohesive\nwhole. The lectures focus on providing the mathematical intuition for\nthe approaches and the assignments provide the much needed practice of\napplication of those concepts.\nApprenticeship has always been an excellent way to gain knowledge as\nthey expose the student to not only learning the craft better but also\nlearning the context. In the classroom, sometimes the context is lost or\nthe analysis is so tunneled that the onus is left on the student to\nfigure out the whole life cycle. Systems thinking of designing authentic\nassessment and equity based grading has given the student experiential\nunderstanding of end-to-end data analysis projects.\nAcknowledgement\nI would like to thank Dr. Yasser Dessouky, Professor and Chair of the\nISE Department for the support and providing space for experimenting\nwith novel pedagogies in Data Science courses. I would also like to\nthank my Faculty Learning Community for engaging discussions about\nauthentic assessment and for stretching my thinking through those\ndiscussions.\nREFERENCES\n\n\n\n[1] 2021. Data\nscience jobs continue to be in-demand. Udacity.\n\n\n[2] Anderson, J.S. and Williams, S.K. 2019. Turning\ndata into better decision making: Asking questions, collecting and\nanalyzing data in a personal analytics project. Decision Sciences\nJournal of Innovative Education. 17, 2 (2019), 126–145.\n\n\n[3] Busteed, B. 2019. Why aren’t graduates ready\nfor work? They’re the least working generation in US history.\nForbes. (2019).\n\n\n[4] Çetinkaya-Rundel, M. and Ellison, V. 2021. A\nfresh look at introductory data science. Journal of Statistics and\nData Science Education. 29, sup1 (2021), S16–S26.\n\n\n[5] Chance, B.L. 1997. Experiences with authentic\nassessment techniques in an introductory statistics course. Journal\nof Statistics Education. 5, 3 (1997).\n\n\n[6] Daley, B.J. 1999. Novice to expert: An\nexploration of how professionals learn. Adult education\nquarterly. 49, 4 (1999), 133–147.\n\n\n[7] Feldman, J. 2018. Grading for equity: What\nit is, why it matters, and how it can transform schools and\nclassrooms. Corwin Press.\n\n\n[8] Garfield, J. and Ben-Zvi, D. 2007. How students\nlearn statistics revisited: A current review of research on teaching and\nlearning statistics. International statistical review. 75, 3\n(2007), 372–396.\n\n\n[9] Garfield, J. and Chance, B. 2000. Assessment in\nstatistics education: Issues and challenges. Mathematical Thinking\nand Learning. 2, 1-2 (2000), 99–125.\n\n\n[10] Harlen, W. and James, M. 1997. Assessment and\nlearning: Differences and relationships between formative and summative\nassessment. Assessment in education: Principles, policy &\npractice. 4, 3 (1997), 365–379.\n\n\n[11] Ijeh, S.B. et al. 2012. How competent\nmathematics teachers develop pedagogical content knowledge in statistics\nteaching. University of Pretoria.\n\n\n[12] Joan B, G. 1994. Beyond testing and grading:\nUsing assessment to improve student learning. Journal of statistics\neducation. 2, 1 (1994).\n\n\n[13] Kirschner, P.A. et al. 2010. Why minimal\nguidance during instruction does not work: An analysis of the failure of\nconstructivist. Based Teaching Work: An Analysis of the Failure of\nConstructivist, Discovery, Problem-Based, Experiential, and\nInquiry-Based Teaching,(November 2014). (2010), 37–41.\n\n\n[14] Libman, Z. 2010. Integrating real-life data\nanalysis in teaching descriptive statistics: A constructivist approach.\nJournal of Statistics Education. 18, 1 (2010).\n\n\n[15] Lucas, B. and Hanson, J. 2016. Thinking like an\nengineer: Using engineering habits of mind and signature pedagogies to\nredesign engineering education. (2016).\n\n\n[16] Schwab-McCoy, A. et al. 2021. Data science in\n2020: Computing, curricula, and challenges for the next 10 years.\nJournal of Statistics and Data Science Education. 29, sup1\n(2021), S40–S50.\n\n\n[17] Staff, T. and Wiggins, A.T.A.G. 2022. 27\ncharacteristics of authentic assessment.\nTeachThought.\n\n\n[18] Tishkovskaya, S. and Lancaster, G.A. 2012.\nStatistical education in the 21st century: A review of challenges,\nteaching innovations and strategies for reform. Journal of\nStatistics Education. 20, 2 (2012).\n\n\n[19] Toikkanen, T. 2016. Focus\non skill mastery, not knowledge acquisition. Medium.\nLifeLearn.\n\n\n[20] Vik Paruchuri Vik is the CEO, A. the author and\nDataquest., F. of 2021. 21\nplaces to find free datasets for data science projects.\nDataquest.\n\n\n[21] Wiggins, G. 1989. A true test:\nToward more authentic and equitable assessment. Phi\nDelta Kappan. 70, 9 (1989), 703–713.\n\n\n[22] Wiggins, G. 1998. Educative\nassessment. Designing Assessments To Inform; Improve Student\nPerformance.\n\n\n[23] Wiggins, G. et al. 2005. Understanding by\ndesign. Ascd.\n\n\n\n\n",
    "preview": "posts/2021-11-01-aa-experiments/conceptmap.png",
    "last_modified": "2022-09-29T14:49:02-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-13-sudoku-pygame/",
    "title": "sudoku-pygame",
    "description": "Experiences in building a sudoku in python with a 8th grader",
    "author": [
      {
        "name": "Shilpa Gupta",
        "url": {}
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\nAs part of More Active Girls in Computing I worked with an 8th grader to build a python game. After much exploring[1], my mentee settled on Sudoku. Now began the real challenge of building the game in a new language in 8 weeks with roughly 24 hours total time. We started with doing an internet search of various Sudoku solutions[2]. Quite a few focused on algorithms for solving Sudoku.\nWhile it was quite attractive to build a solver for Sudoku, we prioritized building a GUI for Sudoku and mimicking solving Sudoku as we would on paper. For that we quickly narrowed to the work of Trevor Appleton. The blog did an excellent job of walking through the various steps. We spent a few weeks going through the code and understanding the pieces. We quickly realized that a. the code only looked at a 9x9 grid with hard coded values and b. It started with a blank puzzle. So for the remaining weeks we focused on making the code generic for a 4x4 grid, adding a GUI for choosing the game, adding a few numbers to have a starter puzzle.\nThe code is documented here https://github.com/guptashilpa/sudoku-pygame\nReferences\n[1]\nhttps://www.pygame.org/docs/tut/MakeGames.html https://www.pygame.org/docs/tut/tom_games2.html#makegames-2-1 https://inventwithpython.com/makinggames.pdf\n[2]\nhttps://norvig.com/sudoku.html https://stackoverflow.com/questions/1697334/algorithm-for-solving-sudoku https://www.geeksforgeeks.org/building-and-visualizing-sudoku-game-using-pygame/ https://www.techwithtim.net/tutorials/python-programming/sudoku-solver-backtracking/ http://www.ams.org/notices/200904/rtx090400460p.pdf https://see.stanford.edu/materials/icspacs106b/H19-RecBacktrackExamples.pdf\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-13T14:42:29-07:00",
    "input_file": {}
  }
]
